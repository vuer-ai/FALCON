# from isaacgym.torch_utils import *
from humanoidverse.utils.torch_utils import (
    generate_sphere_sample_params,
    apply_sphere_sample_to_segments,
    sample_3d_directions,
)

import trimesh


def quat_rotate(q, v):
    """
    Rotate vector(s) v by quaternion(s) q.
    q: (B, 4) - quaternion in (w, x, y, z) format
    v: (B, 3) - vector to rotate
    """
    qvec = q[:, 1:]
    uv = torch.cross(qvec, v, dim=1)
    uuv = torch.cross(qvec, uv, dim=1)
    return v + 2 * (q[:, :1] * uv + uuv)


def quat_rotate_inverse(q, v):
    """
    Rotate vector(s) v by the inverse of quaternion(s) q.
    """
    q_conj = q.clone()
    q_conj[:, 1:] *= -1
    return quat_rotate(q_conj, v)


import torch
from humanoidverse.envs.decoupled_locomotion.decoupled_locomotion_stand_height_waist_wbc_ma_genesis import \
    LeggedRobotDecoupledLocomotionStanceHeightWBC

from isaac_utils.rotations import (
    my_quat_rotate,
)
from humanoidverse.envs.env_utils.visualization import Point

from loguru import logger

DEBUG = True

import time
import functools

# debugging purposes
def time_method(func):
    """Decorator to time any method"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"{func.__name__}: {end - start:.4f}s")
        return result
    return wrapper

def time_all_methods(cls):
    """Class decorator to time all methods"""
    for attr_name in dir(cls):
        attr = getattr(cls, attr_name)
        if callable(attr) and not attr_name.startswith('__'):
            setattr(cls, attr_name, time_method(attr))
    return cls

class LeggedRobotDecoupledLocomotionStanceHeightWBCForce(LeggedRobotDecoupledLocomotionStanceHeightWBC):
    def __init__(self, config, device):
        self.init_done = False
        super().__init__(config, device)
        self.left_hand_link = config.robot.force_control.left_hand_link
        self.right_hand_link = config.robot.force_control.right_hand_link
        self.left_hand_link_index = self.body_names.index(self.left_hand_link)
        self.right_hand_link_index = self.body_names.index(self.right_hand_link)
        logger.info(f"Left Hand Link: {self.left_hand_link}, Index: {self.left_hand_link_index}")
        logger.info(f"Right Hand Link: {self.right_hand_link}, Index: {self.right_hand_link_index}")

        self.left_ankle_dof_indices = [self.dof_names.index(dof) for dof in self.config.robot.left_ankle_dof_names]
        self.right_ankle_dof_indices = [self.dof_names.index(dof) for dof in self.config.robot.right_ankle_dof_names]

        self.j_left_ee = torch.zeros((self.num_envs, 6, 6 + self.num_dofs), device=self.device)
        self.j_right_ee = torch.zeros((self.num_envs, 6, 6 + self.num_dofs), device=self.device)

        if self.config.rewards.get("upper_body_motion_scale_curriculum", False):
            self.action_scale_upper_body = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                      requires_grad=False) * self.config.rewards.upper_body_motion_initial_scale
        else:
            self.action_scale_upper_body = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                      requires_grad=False)
        if self.config.rewards.get("command_height_scale_curriculum", False):
            self.command_height_scale = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                   requires_grad=False) * self.config.rewards.command_height_scale_initial_scale
        else:
            self.command_height_scale = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                   requires_grad=False)
        if self.config.rewards.get("force_scale_curriculum", False):
            self.apply_force_scale = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                requires_grad=False) * self.config.rewards.force_scale_initial_scale
        else:
            self.apply_force_scale = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                requires_grad=False)
        self.zero_tapping_xy_force = self.config.get("zero_tapping_xy_force", False)
        self.joint_effort_limit_scale = self.config.robot.get("dof_effort_limit_scale", 1.0)
        self.joint_effort_limit = torch.tensor(self.config.robot.dof_effort_limit_list[:],
                                               device=self.device, requires_grad=False) * self.joint_effort_limit_scale

        print('----------------------(FORCE CURRICULUM GH1)------------------------')
        print('apply_force_in_physics_step (GH1):', self.config.apply_force_in_physics_step)
        print(f"Left hand link index: {self.left_hand_link_index}")
        print(f"Right hand link index: {self.right_hand_link_index}")

    def _init_force_settings(self):
        # force initialization of value and pos
        self.left_ee_apply_force = torch.zeros((self.num_envs, 3), device=self.device)
        self.right_ee_apply_force = torch.zeros((self.num_envs, 3), device=self.device)
        self.apply_force_tensor = torch.zeros(self.num_envs, self.config.robot.num_bodies, 3, dtype=torch.float,
                                              device=self.device, requires_grad=False)
        self.apply_force_pos_tensor = torch.zeros(self.num_envs, self.config.robot.num_bodies, 3, dtype=torch.float,
                                                  device=self.device, requires_grad=False)
        # force position settings
        self.apply_force_pos_ratio_range = self.config.get("apply_force_pos_ratio_range", [0.0, 1.0])
        self.left_ee_apply_force_pos_ratio = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                        requires_grad=False) * (self.apply_force_pos_ratio_range[0] +
                                                                                self.apply_force_pos_ratio_range[
                                                                                    1]) / 2.0
        self.right_ee_apply_force_pos_ratio = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                         requires_grad=False) * (self.apply_force_pos_ratio_range[0] +
                                                                                 self.apply_force_pos_ratio_range[
                                                                                     1]) / 2.0
        # force application settings
        self.only_apply_z_force_when_walking = self.config.get("only_apply_z_force_when_walking", True)
        self.only_apply_resistance_when_walking = self.config.get("only_apply_resistance_when_walking", True)
        # force ranges
        self.force_xyz_scale = torch.distributions.Dirichlet(torch.tensor([1.0, 1.0, 1.0], device=self.device)).sample(
            (self.num_envs,))
        self.left_force_xyz_scale = torch.distributions.Dirichlet(
            torch.tensor([1.0, 1.0, 1.0], device=self.device)).sample((self.num_envs,))
        self.right_force_xyz_scale = torch.distributions.Dirichlet(
            torch.tensor([1.0, 1.0, 1.0], device=self.device)).sample((self.num_envs,))
        self.force_range_low = torch.zeros(self.num_envs, 3, dtype=torch.float, device=self.device, requires_grad=False)
        self.force_range_high = torch.zeros(self.num_envs, 3, dtype=torch.float, device=self.device,
                                            requires_grad=False)
        self.force_range_low[:, 0] = self.config.apply_force_x_range[0];
        self.force_range_high[:, 0] = self.config.apply_force_x_range[1]
        self.force_range_low[:, 1] = self.config.apply_force_y_range[0];
        self.force_range_high[:, 1] = self.config.apply_force_y_range[1]
        self.force_range_low[:, 2] = self.config.apply_force_z_range[0];
        self.force_range_high[:, 2] = self.config.apply_force_z_range[1]
        # force duration
        self.apply_force_duration = torch.randint(self.config.randomize_force_duration[0],
                                                  self.config.randomize_force_duration[1] + 1, (self.num_envs, 1),
                                                  device=self.device)
        self.left_ee_apply_force_phase = torch.rand((self.num_envs, 1), device=self.device)
        self.right_ee_apply_force_phase = torch.rand((self.num_envs, 1), device=self.device)
        self.left_ee_apply_force_phase_ts = torch.zeros((self.num_envs, 1), device=self.device)
        self.right_ee_apply_force_phase_ts = torch.zeros((self.num_envs, 1), device=self.device)
        # zero force probability
        self.left_zero_force_prob = self.config.get("zero_force_prob", [0.2, 0.2, 0.2])
        self.right_zero_force_prob = self.config.get("zero_force_prob", [0.2, 0.2, 0.2])
        if isinstance(self.left_zero_force_prob, float):
            self.left_zero_force_prob = [self.left_zero_force_prob] * 3
        if isinstance(self.right_zero_force_prob, float):
            self.right_zero_force_prob = [self.right_zero_force_prob] * 3
        self.left_zero_force_prob = torch.tensor(self.left_zero_force_prob, device=self.device)
        self.right_zero_force_prob = torch.tensor(self.right_zero_force_prob, device=self.device)
        self.left_zero_force = (torch.rand((self.num_envs, 3), device=self.device) < self.left_zero_force_prob).float()
        self.right_zero_force = (
                    torch.rand((self.num_envs, 3), device=self.device) < self.right_zero_force_prob).float()
        # random force probability
        self.random_force_prob = self.config.get("random_force_prob", 0.0)
        self.random_force = (torch.rand((self.num_envs, 1), device=self.device) < self.random_force_prob).float()
        # random force pos params
        self.left_fp_unit_dirs = torch.zeros((self.num_envs, 3), device=self.device)
        self.left_fp_radius_scales = torch.zeros((self.num_envs, 1), device=self.device)
        self.right_fp_unit_dirs = torch.zeros((self.num_envs, 3), device=self.device)
        self.right_fp_radius_scales = torch.zeros((self.num_envs, 1), device=self.device)
        # low pass filter for applied force
        self.use_lpf = self.config.get("use_lpf", False)
        self.filtered_left_force_min = torch.zeros((self.num_envs, 3), device=self.device)
        self.filtered_left_force_max = torch.zeros((self.num_envs, 3), device=self.device)
        self.filtered_right_force_min = torch.zeros((self.num_envs, 3), device=self.device)
        self.filtered_right_force_max = torch.zeros((self.num_envs, 3), device=self.device)
        self.force_filter_alpha = self.config.get("force_filter_alpha", 0.2)

        self.extend_curr_pos = torch.zeros((self.num_envs, 3), device=self.device)
        self.max_force_estimation = self.config.get("max_force_estimation", True)
        self.update_apply_force_phase = self.config.get("update_apply_force_phase", False)

    def _init_buffers(self):
        super()._init_buffers()
        # force settings
        self._init_force_settings()
        # stance/random force environments
        self.env_ids_stance = torch.where(self.commands[:, 4] == 0)[0]

        self.upper_body_tracking_sigma = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                    requires_grad=False) * \
                                         (self.config.rewards.upper_body_tracking_sigma_initial_scale \
                                              if self.config.rewards.get("upper_body_tracking_sigma_curriculum", False) \
                                              else self.config.rewards.reward_tracking_sigma.upper_body_dofs)

        self.upper_body_dofs_tracking_reward = torch.zeros(self.num_envs, dtype=torch.float, device=self.device,
                                                           requires_grad=False)

        self.walking_env_mask = (self.commands[:, 4] == 1)
        zero_cmd_mask = torch.all(self.commands[:, :2] == 0, dim=-1)
        self.tapping_env_mask = self.walking_env_mask & zero_cmd_mask
        self.num_tapping_env = torch.sum(self.tapping_env_mask).item()
        # Generate a (num_tapping_env, 1) tensor with 50% probability of [1, 0] and 50% probability of [-1, 0]
        random_choices = torch.randint(0, 2, (self.num_envs, 1), device=self.device)  # Random 0 or 1
        self.random_tapping_dir = torch.where(random_choices == 0, torch.tensor([1, 0], device=self.device),
                                              torch.tensor([-1, 0], device=self.device))

        print("Walking Mask Sum:", self.walking_env_mask.sum().item())
        print("Tapping Env Mask Sum:", self.tapping_env_mask.sum().item())
        print("env_ids_stance:", self.env_ids_stance.shape)

    def _update_tasks_callback(self):
        super()._update_tasks_callback()
        if self.update_apply_force_phase: self._update_apply_force_phase()

    def _update_apply_force_phase(self):
        # For the stance environments, update the phase timestamp
        self.left_ee_apply_force_phase_ts[self.env_ids_stance] += 1.0 / self.apply_force_duration[self.env_ids_stance]
        self.right_ee_apply_force_phase_ts[self.env_ids_stance] += 1.0 / self.apply_force_duration[self.env_ids_stance]
        # self.apply_force_phase_ts[self.env_ids_stance] += self.force_phase_ts_up_or_down[self.env_ids_stance] / self.apply_force_duration[self.env_ids_stance]
        self.left_ee_apply_force_phase[self.env_ids_stance] = abs(
            torch.remainder(self.left_ee_apply_force_phase_ts[self.env_ids_stance], 2.0) - 1.0)
        self.right_ee_apply_force_phase[self.env_ids_stance] = abs(
            torch.remainder(self.right_ee_apply_force_phase_ts[self.env_ids_stance], 2.0) - 1.0)

    def _calculate_max_ee_forces(self):
        # Apply the force at the hand links
        # Compute the linear z-axis jacobian for the left and right hand links
        jacobian = self.simulator.jacobian  # Shape: (num_envs, num_bodies, 6, 35)
        self.j_left_ee = jacobian[:, self.left_hand_link_index, :, :]  # Shape: (num_envs, 6, 35)
        self.j_right_ee = jacobian[:, self.right_hand_link_index, :, :]  # Shape: (num_envs, 6, 35)
        j_left_ee_joint_linear = self.j_left_ee[:, :3, 6:]  # Shape: (num_envs, 3, 29)
        j_right_ee_joint_linear = self.j_right_ee[:, :3, 6:]  # Shape: (num_envs, 3, 29)
        j_left_ee_arm_joint_linear = j_left_ee_joint_linear[:, :, self.left_arm_dof_indices]  # Shape: (num_envs, 3, 7)
        j_right_ee_arm_joint_linear = j_right_ee_joint_linear[:, :,
                                      self.right_arm_dof_indices]  # Shape: (num_envs, 3, 7)
        j_left_ee_waist_joint_linear = j_left_ee_joint_linear[:, :, self.waist_dof_indices]  # Shape: (num_envs, 3, 3)
        j_right_ee_waist_joint_linear = j_right_ee_joint_linear[:, :, self.waist_dof_indices]  # Shape: (num_envs, 3, 3)
        joint_effort_est = self.ref_pd_torques  # Shape: (num_envs, 35)
        # print(f"joint_effort_est: {joint_effort_est}")
        joint_effort_est *= 0.0
        max_delta_joint_effort = self.joint_effort_limit - joint_effort_est
        min_delta_joint_effort = -self.joint_effort_limit - joint_effort_est

        # First do the value, leave the direction for the scaling
        # Yuanhang: Apply X-Y-Z forces to the stance env, and apply only Z force to the walking env
        left_ee_force_min = torch.zeros((self.num_envs, 3), device=self.device)
        left_ee_force_max = torch.zeros((self.num_envs, 3), device=self.device)
        right_ee_force_min = torch.zeros((self.num_envs, 3), device=self.device)
        right_ee_force_max = torch.zeros((self.num_envs, 3), device=self.device)

        left_ee_force_max_delta_joint = torch.mul((1.0 / (torch.abs(j_left_ee_arm_joint_linear) + 1.0e-2)), torch.stack(
            [max_delta_joint_effort[:, self.left_arm_dof_indices]] * 3, dim=1))  # Shape: (num_envs, 3, 7)
        left_ee_force_min_delta_joint = torch.mul((1.0 / (torch.abs(j_left_ee_arm_joint_linear) + 1.0e-2)), torch.stack(
            [min_delta_joint_effort[:, self.left_arm_dof_indices]] * 3, dim=1))
        right_ee_force_max_delta_joint = torch.mul((1.0 / (torch.abs(j_right_ee_arm_joint_linear) + 1.0e-2)),
                                                   torch.stack(
                                                       [max_delta_joint_effort[:, self.right_arm_dof_indices]] * 3,
                                                       dim=1))  # Shape: (num_envs, 3, 7)
        right_ee_force_min_delta_joint = torch.mul((1.0 / (torch.abs(j_right_ee_arm_joint_linear) + 1.0e-2)),
                                                   torch.stack(
                                                       [min_delta_joint_effort[:, self.right_arm_dof_indices]] * 3,
                                                       dim=1))
        left_ee_force_min = torch.max(left_ee_force_min_delta_joint, dim=2)[0]
        left_ee_force_max = torch.min(left_ee_force_max_delta_joint, dim=2)[0]
        right_ee_force_min = torch.max(right_ee_force_min_delta_joint, dim=2)[0]
        right_ee_force_max = torch.min(right_ee_force_max_delta_joint, dim=2)[0]
        left_ee_force_min = torch.cat([left_ee_force_min[:, 0:1] * self.force_xyz_scale[:, 0:1],
                                       left_ee_force_min[:, 1:2] * self.force_xyz_scale[:, 1:2],
                                       left_ee_force_min[:, 2:3] * self.force_xyz_scale[:, 2:3]], dim=1)
        left_ee_force_max = torch.cat([left_ee_force_max[:, 0:1] * self.force_xyz_scale[:, 0:1],
                                       left_ee_force_max[:, 1:2] * self.force_xyz_scale[:, 1:2],
                                       left_ee_force_max[:, 2:3] * self.force_xyz_scale[:, 2:3]], dim=1)
        right_ee_force_min = torch.cat([right_ee_force_min[:, 0:1] * self.force_xyz_scale[:, 0:1],
                                        right_ee_force_min[:, 1:2] * self.force_xyz_scale[:, 1:2],
                                        right_ee_force_min[:, 2:3] * self.force_xyz_scale[:, 2:3]], dim=1)
        right_ee_force_max = torch.cat([right_ee_force_max[:, 0:1] * self.force_xyz_scale[:, 0:1],
                                        right_ee_force_max[:, 1:2] * self.force_xyz_scale[:, 1:2],
                                        right_ee_force_max[:, 2:3] * self.force_xyz_scale[:, 2:3]], dim=1)

        if self.only_apply_z_force_when_walking:
            walk_envs_idx = torch.where(self.commands[:, 4] == 1)[0]
            if len(walk_envs_idx) > 0:
                idx = walk_envs_idx
                left_ee_force_max[idx] = torch.cat([torch.zeros_like(left_ee_force_max[idx, 0:1]),
                                                    torch.zeros_like(left_ee_force_max[idx, 1:2]),
                                                    left_ee_force_max[idx, 2:3]], dim=1)
                left_ee_force_min[idx] = torch.cat([torch.zeros_like(left_ee_force_min[idx, 0:1]),
                                                    torch.zeros_like(left_ee_force_min[idx, 1:2]),
                                                    left_ee_force_min[idx, 2:3]], dim=1)
                right_ee_force_max[idx] = torch.cat([torch.zeros_like(right_ee_force_max[idx, 0:1]),
                                                     torch.zeros_like(right_ee_force_max[idx, 1:2]),
                                                     right_ee_force_max[idx, 2:3]], dim=1)
                right_ee_force_min[idx] = torch.cat([torch.zeros_like(right_ee_force_min[idx, 0:1]),
                                                     torch.zeros_like(right_ee_force_min[idx, 1:2]),
                                                     right_ee_force_min[idx, 2:3]], dim=1)

        # if self.is_evaluating: print(f"Max EE Force, Z: {ee_force_max_z}")
        # print(f"Max EE Force: {left_ee_force_max}, Min EE Force: {left_ee_force_min}, Force XYZ Scale: {self.left_force_xyz_scale}")

        if self.use_lpf:
            self.filtered_left_force_min = self.force_filter_alpha * left_ee_force_min + (
                        1 - self.force_filter_alpha) * self.filtered_left_force_min
            self.filtered_left_force_max = self.force_filter_alpha * left_ee_force_max + (
                        1 - self.force_filter_alpha) * self.filtered_left_force_max
            self.filtered_right_force_min = self.force_filter_alpha * right_ee_force_min + (
                        1 - self.force_filter_alpha) * self.filtered_right_force_min
            self.filtered_right_force_max = self.force_filter_alpha * right_ee_force_max + (
                        1 - self.force_filter_alpha) * self.filtered_right_force_max
            left_ee_force_min = self.filtered_left_force_min
            left_ee_force_max = self.filtered_left_force_max
            right_ee_force_min = self.filtered_right_force_min
            right_ee_force_max = self.filtered_right_force_max

        return left_ee_force_min, left_ee_force_max, right_ee_force_min, right_ee_force_max, j_left_ee_waist_joint_linear, j_right_ee_waist_joint_linear

    def _scale_forces(self,
                      left_ee_force_min, left_ee_force_max,
                      right_ee_force_min, right_ee_force_max,
                      j_left_ee_waist_joint_linear, j_right_ee_waist_joint_linear):
        # Apply different force phase to left and right EE
        # left_ee_force_phased = self.force_range_low + (self.force_range_high - self.force_range_low) * self.left_ee_apply_force_phase
        # right_ee_force_phased = self.force_range_low + (self.force_range_high - self.force_range_low) * self.right_ee_apply_force_phase
        left_ee_force_phased = left_ee_force_min + (
                    left_ee_force_max - left_ee_force_min) * self.left_ee_apply_force_phase
        right_ee_force_phased = right_ee_force_min + (
                    right_ee_force_max - right_ee_force_min) * self.right_ee_apply_force_phase

        # Apply the force scale and the force phase to the maximum force norm
        left_hand_force = left_ee_force_phased * self.apply_force_scale + torch.rand((self.num_envs, 3),
                                                                                     device=self.device)  # Shape: (num_envs, 3)
        right_hand_force = right_ee_force_phased * self.apply_force_scale + torch.rand((self.num_envs, 3),
                                                                                       device=self.device)  # Shape: (num_envs, 3)

        # Zero the force if zero force probability is met
        left_hand_force *= (1 - self.left_zero_force)
        right_hand_force *= (1 - self.right_zero_force)

        # Clip using the min/max as bounds
        left_hand_force = torch.clip(left_hand_force, self.force_range_low, self.force_range_high)
        right_hand_force = torch.clip(right_hand_force, self.force_range_low, self.force_range_high)

        # Apply waist joint constraint as post-processing
        # if j_left_ee_waist_joint_linear is not None and j_right_ee_waist_joint_linear is not None:
        #     left_ee_torque_on_waist = j_left_ee_waist_joint_linear.transpose(1, 2).bmm(left_hand_force.unsqueeze(-1)).squeeze(-1)
        #     right_ee_torque_on_waist = j_right_ee_waist_joint_linear.transpose(1, 2).bmm(right_hand_force.unsqueeze(-1)).squeeze(-1)
        #     total_waist_torque = left_ee_torque_on_waist + right_ee_torque_on_waist
        #     scaling_factor = torch.min(torch.ones_like(total_waist_torque),
        #                             self.joint_effort_limit[self.waist_dof_indices] / (total_waist_torque.abs() + 1e-6))
        #     if scaling_factor.min() < 1.0 and self.is_evaluating:
        #         print(f"Scaling Factor: {scaling_factor.min()}")
        #     left_hand_force *= scaling_factor
        #     right_hand_force *= scaling_factor
        # print("Apply Force Tensor (non-zero):", torch.nonzero(self.apply_force_tensor.norm(dim=-1)))
        # print("Left Force Sample:", self.apply_force_tensor[0, self.left_hand_link_index])
        # print("Right Force Sample:", self.apply_force_tensor[0, self.right_hand_link_index])

        return left_hand_force, right_hand_force

    def _calculate_ee_forces(self):
        if self.max_force_estimation:
            left_ee_force_min, left_ee_force_max, \
                right_ee_force_min, right_ee_force_max, \
                j_left_ee_waist_joint_linear, j_right_ee_waist_joint_linear = self._calculate_max_ee_forces()
        else:
            left_ee_force_min = right_ee_force_min = self.force_range_low
            left_ee_force_max = right_ee_force_max = self.force_range_high
            j_left_ee_waist_joint_linear = j_right_ee_waist_joint_linear = None
        # print(f"Left EE Force Min: {left_ee_force_min}, Max: {left_ee_force_max}")
        # print(f"Right EE Force Min: {right_ee_force_min}, Max: {right_ee_force_max}")
        left_hand_force, right_hand_force = self._scale_forces(left_ee_force_min, left_ee_force_max,
                                                               right_ee_force_min, right_ee_force_max,
                                                               j_left_ee_waist_joint_linear,
                                                               j_right_ee_waist_joint_linear)

        if self.is_evaluating:
            self.walking_env_mask = (self.commands[:, 4] == 1)
            zero_cmd_mask = torch.all(self.commands[:, :2] == 0, dim=-1)
            self.tapping_env_mask = self.walking_env_mask & zero_cmd_mask
            self.num_tapping_env = torch.sum(self.tapping_env_mask).item()
            # left_hand_force = torch.zeros_like(left_hand_force)
            # right_hand_force = torch.zeros_like(right_hand_force)
            # left_hand_force[:, 0] = -30
            # right_hand_force[:, 0] = -30
            pass

        # Get the walking direction
        walking_dir = quat_rotate(self.base_quat,
                                  torch.cat([self.commands[:, :2],
                                             torch.zeros((self.num_envs, 1), device=self.device)],
                                            dim=-1))[:, :2]  # (batch_size, 2)
        walking_dir_norm = torch.norm(walking_dir, dim=-1, keepdim=True) + 1e-6
        walking_dir[self.tapping_env_mask] = quat_rotate(self.base_quat[self.tapping_env_mask],
                                                         torch.cat([self.random_tapping_dir[self.tapping_env_mask],
                                                                    torch.zeros((self.num_tapping_env, 1),
                                                                                device=self.device)],
                                                                   dim=-1))[:, :2]
        walking_dir_norm[self.tapping_env_mask] = torch.norm(walking_dir[self.tapping_env_mask], dim=-1,
                                                             keepdim=True) + 1e-6
        if self.zero_tapping_xy_force:
            walking_dir[self.tapping_env_mask] *= 0.0
        if self.only_apply_resistance_when_walking:
            walking_dir_unit = torch.zeros_like(walking_dir)
            walking_dir_unit[self.walking_env_mask] = -walking_dir[self.walking_env_mask] / walking_dir_norm[
                self.walking_env_mask]

            # Get the x-y components of the force
            left_hand_force_xy = left_hand_force[:, :2]
            right_hand_force_xy = right_hand_force[:, :2]

            left_hand_force_proj = left_hand_force_xy.clone()
            right_hand_force_proj = right_hand_force_xy.clone()

            left_hand_force_proj[self.walking_env_mask] = torch.abs(
                torch.sum(left_hand_force_xy[self.walking_env_mask] * walking_dir_unit[self.walking_env_mask],
                          dim=-1, keepdim=True)) * walking_dir_unit[self.walking_env_mask]
            right_hand_force_proj[self.walking_env_mask] = torch.abs(
                torch.sum(right_hand_force_xy[self.walking_env_mask] * walking_dir_unit[self.walking_env_mask],
                          dim=-1, keepdim=True)) * walking_dir_unit[self.walking_env_mask]

            # Combine the x-y components with the z component
            left_hand_force = torch.cat([left_hand_force_proj, left_hand_force[:, 2:3]], dim=-1)
            right_hand_force = torch.cat([right_hand_force_proj, right_hand_force[:, 2:3]], dim=-1)

        self.left_ee_apply_force = quat_rotate_inverse(self.base_quat, left_hand_force.clone())
        self.right_ee_apply_force = quat_rotate_inverse(self.base_quat, right_hand_force.clone())

        # Apply the force to the hand links
        self.apply_force_tensor[:, self.left_hand_link_index, :] = left_hand_force
        self.apply_force_tensor[:, self.right_hand_link_index, :] = right_hand_force

    def _apply_force_in_physics_step(self):
        # Apply the force in the physics step
        self.torques = self._compute_torques(self.actions_after_delay).view(self.torques.shape)

        if self.config.apply_force_in_physics_step:
            self.simulator.apply_rigid_body_force_at_pos_tensor(self.apply_force_tensor, self.apply_force_pos_tensor)

        # print("Force Pos Left:", self.apply_force_pos_tensor[0, self.left_hand_link_index])
        # print("Force Pos Right:", self.apply_force_pos_tensor[0, self.right_hand_link_index])

        self.simulator.apply_torques_at_dof(self.torques)

    def _physics_step(self):
        self.render()
        self._calculate_ee_forces()
        for _ in range(self.config.simulator.config.sim.control_decimation):
            self._apply_force_in_physics_step()
            self.simulator.simulate_at_each_physics_step()

    def _resample_force_settings(self, env_ids):
        # Yuanhang: use Dirichlet distribution to sample the force at maximum and minimum
        if env_ids.numel() > 0:  # Only update if env_ids is not empty
            self.left_force_xyz_scale[env_ids] = torch.distributions.Dirichlet(
                torch.tensor([1.0, 1.0, 1.0], device=self.device)
            ).sample((len(env_ids),))
            self.right_force_xyz_scale[env_ids] = torch.distributions.Dirichlet(
                torch.tensor([1.0, 1.0, 1.0], device=self.device)
            ).sample((len(env_ids),))
            self.force_xyz_scale[env_ids] = torch.distributions.Dirichlet(
                torch.tensor([1.0, 1.0, 1.0], device=self.device)
            ).sample((len(env_ids),))
        self.apply_force_duration[env_ids] = torch.randint(self.config.randomize_force_duration[0],
                                                           self.config.randomize_force_duration[1] + 1,
                                                           (len(env_ids), 1), device=self.device)
        self.left_ee_apply_force_phase_ts[env_ids] = torch.rand((len(env_ids), 1), device=self.device)
        self.right_ee_apply_force_phase_ts[env_ids] = torch.rand((len(env_ids), 1), device=self.device)
        self.left_ee_apply_force_phase[env_ids] = torch.rand((len(env_ids), 1), device=self.device)
        self.right_ee_apply_force_phase[env_ids] = torch.rand((len(env_ids), 1), device=self.device)
        self.left_zero_force[env_ids] = (
                    torch.rand((len(env_ids), 3), device=self.device) < self.left_zero_force_prob).float()
        self.right_zero_force[env_ids] = (
                    torch.rand((len(env_ids), 3), device=self.device) < self.right_zero_force_prob).float()
        self.random_force[env_ids] = (
                    torch.rand((len(env_ids), 1), device=self.device) < self.random_force_prob).float()

        # Resample the apply force sphere params
        self.left_fp_unit_dirs[env_ids], self.left_fp_radius_scales[env_ids] = generate_sphere_sample_params(
            len(env_ids), self.device)
        self.right_fp_unit_dirs[env_ids], self.right_fp_radius_scales[env_ids] = generate_sphere_sample_params(
            len(env_ids), self.device)
        self.left_ee_apply_force_pos_ratio[env_ids] = torch.rand((len(env_ids), 1), device=self.device) * (
                    self.apply_force_pos_ratio_range[1] - self.apply_force_pos_ratio_range[0]) + \
                                                      self.apply_force_pos_ratio_range[0]
        self.right_ee_apply_force_pos_ratio[env_ids] = torch.rand((len(env_ids), 1), device=self.device) * (
                    self.apply_force_pos_ratio_range[1] - self.apply_force_pos_ratio_range[0]) + \
                                                       self.apply_force_pos_ratio_range[0]

    def resample_motion(self):
        self._motion_lib.load_motions(random_sample=True)
        # Yuanhang: do not reset the envs, otherwise the episode lengths conflict with the ppo buffer
        # self.reset_envs_idx(torch.arange(self.num_envs, device=self.device))
        # Yuanhang: reset the filtered forces because the motion is changed suddenly
        self.filtered_left_force_max *= 0.0
        self.filtered_left_force_min *= 0.0
        self.filtered_right_force_max *= 0.0
        self.filtered_right_force_min *= 0.0

    def _resample_motion_times(self, env_ids):
        # return
        if len(env_ids) == 0:
            return
        self.motion_len[env_ids] = self._motion_lib.get_motion_length(self.motion_ids[env_ids])
        if self.is_evaluating:
            self.motion_start_times[env_ids] = torch.zeros(len(env_ids), dtype=torch.float32, device=self.device)
        else:
            self.motion_start_times[env_ids] = self._motion_lib.sample_time(self.motion_ids[env_ids])
        # Yuanhang: reset the filtered forces because the motion is changed suddenly
        self.filtered_left_force_max[env_ids] *= 0.0
        self.filtered_left_force_min[env_ids] *= 0.0
        self.filtered_right_force_max[env_ids] *= 0.0
        self.filtered_right_force_min[env_ids] *= 0.0

    def _resample_commands(self, env_ids):
        super()._resample_commands(env_ids)
        # Resample the force settings for the environments
        self._resample_force_settings(env_ids)

        self.walking_env_mask = (self.commands[:, 4] == 1)
        zero_cmd_mask = torch.all(self.commands[:, :2] == 0, dim=-1)
        self.tapping_env_mask = self.walking_env_mask & zero_cmd_mask
        self.num_tapping_env = torch.sum(self.tapping_env_mask).item()
        # Restrict to resample only for tapping envs within envs_id
        tapping_envs_to_resample = self.tapping_env_mask[env_ids]
        tapping_indices = env_ids[tapping_envs_to_resample]
        # Generate random directions for them
        random_choices = torch.randint(0, 2, (tapping_indices.shape[0], 1), device=self.device)  # Random 0 or 1
        self.random_tapping_dir[tapping_indices] = torch.where(random_choices == 0,
                                                               torch.tensor([1, 0], device=self.device),
                                                               torch.tensor([-1, 0], device=self.device))

        if len(env_ids) != 0:
            self.env_ids_stance = torch.where(self.commands[env_ids, 4] == 0)[0]

    def set_is_evaluating(self, command=None):
        super().set_is_evaluating()
        if self.config.get('analysis', False):
            self._resample_commands(torch.arange(self.num_envs, device=self.device))
        else:
            self.commands = torch.zeros((self.num_envs, 9), dtype=torch.float32, device=self.device)
            self.commands[:, 8] = self.config.rewards.desired_base_height
            if command is not None:
                self.commands[:, :3] = torch.tensor(command).to(self.device)  # only set the first 3 commands

            # stance/random force environments
            self.env_ids_stance = torch.where(self.commands[:, 4] == 0)[0]

            self.left_ee_apply_force_pos_ratio = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                            requires_grad=False) * (
                                                             self.apply_force_pos_ratio_range[0] +
                                                             self.apply_force_pos_ratio_range[1]) / 2.0
            self.right_ee_apply_force_pos_ratio = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                             requires_grad=False) * (
                                                              self.apply_force_pos_ratio_range[0] +
                                                              self.apply_force_pos_ratio_range[1]) / 2.0

            self.left_force_xyz_scale = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float, device=self.device).repeat(
                self.num_envs, 1).requires_grad_(False)
            self.right_force_xyz_scale = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float, device=self.device).repeat(
                self.num_envs, 1).requires_grad_(False)
        # Apply full upper body action scale
        self.action_scale_upper_body = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                  requires_grad=False)
        self.command_height_scale = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                               requires_grad=False)
        # Apply force scale
        force_level = self.config.get("eval_force_level", 0)
        if force_level == 0:
            self.apply_force_scale = torch.zeros(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                 requires_grad=False)
        elif force_level == 1:
            self.apply_force_scale = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                requires_grad=False) / 2
        elif force_level == 2:
            self.apply_force_scale = torch.ones(self.num_envs, 1, dtype=torch.float, device=self.device,
                                                requires_grad=False)

        print("Apply Force Scale (sample):", self.apply_force_scale[:5].flatten())
        assert torch.all((self.apply_force_scale >= self.config.rewards.force_scale_min) & (
                    self.apply_force_scale <= self.config.rewards.force_scale_max))

        # Hardcode
        self.only_apply_z_force_when_walking = False
        self.only_apply_resistance_when_walking = True
        self.motion_start_times *= 0.0
        self.max_force_estimation = True
        self.update_apply_force_phase = True
        self.use_lpf = True
        # self.config.obs.noise_scales = {
        #     key: value * 0.0 for key, value in self.config.obs.noise_scales.items()
        # }
        # print("noise scales: ", self.config.obs.noise_scales)

    def reset_envs_idx(self, env_ids, target_states=None, target_buf=None):
        """ Resets the environments with the given ids and optionally to the target states
        """
        if len(env_ids) == 0:
            return
        self.need_to_refresh_envs[env_ids] = True
        # avoid updating command curriculum at each step since the maximum command is common to all envs
        # if self.config.commands.curriculum and (self.common_step_counter % self.max_episode_length==0):
        #     self.update_command_curriculum(env_ids)
        if self.config.rewards.upper_body_motion_scale_curriculum:
            self._update_upper_body_motion_scale_curriculum(env_ids)
        else:
            self.action_scale_upper_body[env_ids] = 1.0
        if self.config.rewards.get("force_scale_curriculum", False):
            self._update_force_scale_curriculum(env_ids)
        if self.config.rewards.get("command_height_scale_curriculum", False):
            self._update_command_height_curriculum(env_ids)
        else:
            self.command_height_scale[env_ids] = 1.0
        if self.config.rewards.get("upper_body_tracking_sigma_curriculum", False):
            self._update_upper_body_tracking_sigma_curriculum(env_ids)
        self._reset_buffers_callback(env_ids, target_buf)
        self._reset_tasks_callback(env_ids)  # if target_states is not None, reset to target states
        self._reset_robot_states_callback(env_ids, target_states)

        self.upper_body_dofs_tracking_reward[env_ids] *= 0.0

        self.filtered_left_force_max[env_ids] *= 0.0
        self.filtered_left_force_min[env_ids] *= 0.0
        self.filtered_right_force_max[env_ids] *= 0.0
        self.filtered_right_force_min[env_ids] *= 0.0

        # fill extras
        self.extras["episode"] = {}
        for key in self.episode_sums.keys():
            self.extras["episode"]['rew_' + key] = torch.mean(
                self.episode_sums[key][env_ids]) / self.max_episode_length_s
            self.episode_sums[key][env_ids] = 0.
        self.extras["time_outs"] = self.time_out_buf
        # self._refresh_sim_tensors()

    def _pre_compute_observations_callback(self):
        super()._pre_compute_observations_callback()
        B = self.motion_ids.shape[0]
        ################### EXTEND Rigid body POS #####################
        rotated_pos_in_parent = my_quat_rotate(
            self.simulator._rigid_body_rot[:, self.extend_body_parent_ids].reshape(-1, 4),
            self.extend_body_pos_in_parent.reshape(-1, 3)
        )
        self.extend_curr_pos = my_quat_rotate(
            self.extend_body_rot_in_parent_xyzw.reshape(-1, 4),
            rotated_pos_in_parent
        ).view(self.num_envs, -1, 3) + self.simulator._rigid_body_pos[:, self.extend_body_parent_ids]
        self._rigid_body_pos_extend = torch.cat([self.simulator._rigid_body_pos, self.extend_curr_pos], dim=1)
        self.marker_coords[:] = self._rigid_body_pos_extend.reshape(B, -1, 3)  # visualize the robot rigid body pos
        # self.marker_coords[:] = self.ref_body_pos_extend.reshape(B, -1, 3) # visualize the target rigid body pos
        # Hardcode: decide which one to use
        # 1. Apply forces within randomly sample shperes
        # left_ee_apply_force_pos = apply_sphere_sample_to_segments(self.simulator._rigid_body_pos[:, self.left_hand_link_index, :],
        #                                                           self.extend_curr_pos[:, 0, :],
        #                                                           self.left_fp_unit_dirs, self.left_fp_radius_scales)
        # right_ee_apply_force_pos = apply_sphere_sample_to_segments(self.simulator._rigid_body_pos[:, self.right_hand_link_index, :],
        #                                                            self.extend_curr_pos[:, 1, :],
        #                                                            self.right_fp_unit_dirs, self.right_fp_radius_scales)

        # 2. Apply force between the hand link and the sampled sphere
        left_ee_apply_force_pos = (self.extend_curr_pos[:, 0, :] - self.simulator._rigid_body_pos[:,
                                                                   self.left_hand_link_index,
                                                                   :]) * self.left_ee_apply_force_pos_ratio + self.simulator._rigid_body_pos[
                                                                                                              :,
                                                                                                              self.left_hand_link_index,
                                                                                                              :]
        right_ee_apply_force_pos = (self.extend_curr_pos[:, 1, :] - self.simulator._rigid_body_pos[:,
                                                                    self.right_hand_link_index,
                                                                    :]) * self.right_ee_apply_force_pos_ratio + self.simulator._rigid_body_pos[
                                                                                                                :,
                                                                                                                self.right_hand_link_index,
                                                                                                                :]
        # 3. Apply force at the hand links
        # self.apply_force_pos_tensor[:, self.left_hand_link_index,:] = self.simulator._rigid_body_pos[:, self.left_hand_link_index, :]
        # self.apply_force_pos_tensor[:, self.right_hand_link_index,:] = self.simulator._rigid_body_pos[:, self.right_hand_link_index, :]
        self.apply_force_pos_tensor[:, self.left_hand_link_index, :] = left_ee_apply_force_pos
        self.apply_force_pos_tensor[:, self.right_hand_link_index, :] = right_ee_apply_force_pos
        if self.is_evaluating:
            # self.ref_upper_dof_pos *= 0
            pass

    def _draw_debug_vis(self):
        self.simulator.clear_lines()
        self._refresh_sim_tensors()

        for env_id in range(self.num_envs):
            # for pos_id, pos_joint in enumerate(self.marker_coords[env_id]): # idx 0 torso (duplicate with 11)
            #     if self.config.robot.motion.visualization.customize_color:
            #         color_inner = self.config.robot.motion.visualization.marker_joint_colors[pos_id % len(self.config.robot.motion.visualization.marker_joint_colors)]
            #     else:
            #         color_inner = (0.3, 0.3, 0.3)
            #     color_inner = tuple(color_inner)

            #     self.simulator.draw_sphere(pos_joint, 0.04, color_inner, env_id)
            # draw forces

            force_left_hand = self.apply_force_tensor[env_id, self.left_hand_link_index, :]
            force_right_hand = self.apply_force_tensor[env_id, self.right_hand_link_index, :]

            force_pos_left_hand = self.apply_force_pos_tensor[env_id, self.left_hand_link_index, :]
            force_pos_right_hand = self.apply_force_pos_tensor[env_id, self.right_hand_link_index, :]

            force_list = [force_left_hand, force_right_hand]
            pos_list = [force_pos_left_hand, force_pos_right_hand]
            force_mag_list = [0.025, 0.025]
            color_schems = [(0.851, 0.144, 0.07), (0.851, 0.144, 0.07)]
            # color_schems = [(0., .5, 0.), (0., .5, 0.)]
            line_widths = [0.02, 0.02]

            # print("Force Tensor Left:", self.apply_force_tensor[env_id, self.left_hand_link_index])
            # print("Force Pos Left:", self.apply_force_pos_tensor[env_id, self.left_hand_link_index])

            for force, pos, force_mag, color, line_width in zip(force_list, pos_list, force_mag_list, color_schems,
                                                                line_widths):
                for _ in range(20):
                    # start_point = pos + torch.rand(3, device=self.device) * line_width
                    start_point = pos  # Fixed origin
                    end_point = pos + force * force_mag

                    # print("pos:", pos)
                    # print("force:", force)
                    # print("start_point:", start_point)
                    # print("end_point:", end_point)
                    self.simulator.draw_line(Point(start_point + torch.rand(3, device=self.device) * line_width),
                                             Point(end_point + torch.rand(3, device=self.device) * line_width),
                                             Point(color),
                                             env_id)

    ############################ Curriculum #############################
    def _update_upper_body_tracking_sigma_curriculum(self, env_ids):
        """ Implement upper body tracking sigma curriculum based on the tracking performance
        """
        upper_body_tracking_error = self.upper_body_dofs_tracking_reward[env_ids] / self.episode_length_buf[env_ids]
        env_ids_scale_up_mask = upper_body_tracking_error < self.config.rewards.upper_body_tracking_sigma_scale_up_threshold
        env_ids_scale_up = env_ids[torch.where(env_ids_scale_up_mask)[0]]
        env_ids_scale_down_mask = upper_body_tracking_error > self.config.rewards.upper_body_tracking_sigma_scale_down_threshold
        env_ids_scale_down = env_ids[torch.where(env_ids_scale_down_mask)[0]]
        self.upper_body_tracking_sigma[env_ids_scale_up] += self.config.rewards.upper_body_tracking_sigma_scale_up
        self.upper_body_tracking_sigma[env_ids_scale_down] -= self.config.rewards.upper_body_tracking_sigma_scale_down
        # Clip the scale
        self.upper_body_tracking_sigma[env_ids] = torch.clip(self.upper_body_tracking_sigma[env_ids],
                                                             self.config.rewards.upper_body_tracking_sigma_min,
                                                             self.config.rewards.upper_body_tracking_sigma_max)

    def _update_force_scale_curriculum(self, env_ids):
        """ Implement force curriculum

        Args:
            env_ids (List[int]): ids of environments being reset
        """
        env_ids_scale_up_mask = self.episode_length_buf[env_ids] > self.config.rewards.force_scale_up_threshold
        # env_ids_scale_up_mask = self.upper_body_dofs_tracking_reward[env_ids] / self.episode_length_buf[env_ids] > self.config.rewards.force_scale_up_threshold
        env_ids_scale_up = env_ids[torch.where(env_ids_scale_up_mask)[0]]
        env_ids_scale_down_mask = self.episode_length_buf[env_ids] < self.config.rewards.force_scale_down_threshold
        # env_ids_scale_down_mask = self.upper_body_dofs_tracking_reward[env_ids] / self.episode_length_buf[env_ids] < self.config.rewards.force_scale_down_threshold
        env_ids_scale_down = env_ids[torch.where(env_ids_scale_down_mask)[0]]
        self.apply_force_scale[env_ids_scale_up] += self.config.rewards.force_scale_up
        self.apply_force_scale[env_ids_scale_down] -= self.config.rewards.force_scale_down
        # self.apply_force_scale[env_ids_scale_up] *= (1 + self.config.rewards.reward_penalty_degree)
        # self.apply_force_scale[env_ids_scale_down] *= (1 - self.config.rewards.reward_penalty_degree)
        # Clip the scale
        self.apply_force_scale[env_ids] = torch.clip(self.apply_force_scale[env_ids],
                                                     self.config.rewards.force_scale_min,
                                                     self.config.rewards.force_scale_max)

    ########################### FEET REWARDS ###########################

    def _reward_tracking_upper_body_dofs(self):
        # Reward the difference between the waist dof pos and the reference
        upper_body_pos = self.simulator.dof_pos[:, self.upper_dof_indices]
        upper_body_dofs_error = torch.sum(torch.square(upper_body_pos - self.ref_upper_dof_pos), dim=1)
        upper_body_dofs_tracking_reward = torch.exp(
            -upper_body_dofs_error / (self.upper_body_tracking_sigma.squeeze(-1)))
        self.upper_body_dofs_tracking_reward += upper_body_dofs_tracking_reward
        return upper_body_dofs_tracking_reward

    def _reward_tracking_stance_base_height(self):
        # Tracking of base height commands (z axe)
        base_height_error = torch.abs(self.commands[:, 8] - self.simulator.robot_root_states[:, 2])
        return torch.exp(-base_height_error / self.config.rewards.reward_tracking_sigma.base_height) * (
                    1 - self.commands[:, 4])  # only apply the base height penalty if stance

    def _reward_tracking_walk_base_height(self):
        # Tracking of base height commands (z axe)
        total_apply_force = torch.norm(
            self.apply_force_tensor[:, self.left_hand_link_index, :] + self.apply_force_tensor[:,
                                                                       self.right_hand_link_index, :], dim=1)
        base_height_error = torch.abs(self.commands[:, 8] - self.simulator.robot_root_states[:, 2]) * (
                    1 - torch.clip(total_apply_force / 50, 0, 1))
        return torch.exp(-base_height_error / self.config.rewards.reward_tracking_sigma.base_height) * self.commands[:,
                                                                                                       4]

    def _reward_penalty_ankle_roll(self):
        # Compute the penalty for ankle roll
        left_ankle_roll = self.simulator.dof_pos[:, self.left_ankle_dof_indices[1:2]]
        right_ankle_roll = self.simulator.dof_pos[:, self.right_ankle_dof_indices[1:2]]
        return torch.sum(torch.abs(left_ankle_roll) + torch.abs(right_ankle_roll),
                         dim=1)  # * (1 - self.commands[:, 4]) # only apply the ankle roll penalty if stance

    def _reward_penalty_stance_feet_vel(self):
        # Penalize the velocity of the stance feet
        left_ee_lin_vel = self.simulator._rigid_body_vel[:, self.left_hand_link_index, 0:3]
        left_ee_ang_vel = self.simulator._rigid_body_ang_vel[:, self.left_hand_link_index, 0:3]
        left_ee_vel = torch.cat([left_ee_lin_vel, left_ee_ang_vel], dim=1)
        right_ee_lin_vel = self.simulator._rigid_body_vel[:, self.right_hand_link_index, 0:3]
        right_ee_ang_vel = self.simulator._rigid_body_ang_vel[:, self.right_hand_link_index, 0:3]
        right_ee_vel = torch.cat([right_ee_lin_vel, right_ee_ang_vel], dim=1)
        return (torch.norm(left_ee_vel, dim=1) + torch.norm(right_ee_vel, dim=1)) * (1 - self.commands[:, 4])

    def _reward_penalty_ee_lin_acc(self):
        # Penalize the end effector acceleration
        left_ee_lin_acc = self.simulator._rigid_body_vel[:, self.left_hand_link_index, 0:3] - self.last_left_ee_vel
        right_ee_lin_acc = self.simulator._rigid_body_vel[:, self.right_hand_link_index, 0:3] - self.last_right_ee_vel
        end_effector_acc = torch.cat([left_ee_lin_acc.unsqueeze(1), right_ee_lin_acc.unsqueeze(1)], dim=1)
        end_effector_acc_norm = torch.norm(end_effector_acc, dim=2)
        return torch.sum(end_effector_acc_norm, dim=1)

    def _reward_penalty_ee_ang_acc(self):
        # Penalize the end effector angular acceleration
        left_ee_ang_acc = self.simulator._rigid_body_ang_vel[:, self.left_hand_link_index,
                          0:3] - self.last_left_ee_ang_vel
        right_ee_ang_acc = self.simulator._rigid_body_ang_vel[:, self.right_hand_link_index,
                           0:3] - self.last_right_ee_ang_vel
        end_effector_ang_acc = torch.cat([left_ee_ang_acc.unsqueeze(1), right_ee_ang_acc.unsqueeze(1)], dim=1)
        end_effector_ang_acc_norm = torch.norm(end_effector_ang_acc, dim=2)
        return torch.sum(end_effector_ang_acc_norm, dim=1)

    ######################### Observations #########################
    def _get_obs_left_ee_apply_force(self):
        # return the force exerted on the left ee (hand)
        # print(f"Apply Force Tensor: {self.apply_force_tensor[:, self.left_hand_link_index, 2:3]}")
        return self.left_ee_apply_force

    def _get_obs_right_ee_apply_force(self):
        # return the force exerted on the right ee (hand)
        return self.right_ee_apply_force
